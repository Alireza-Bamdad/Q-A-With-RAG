import json
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
import faiss
import streamlit as st
import requests
import os

# Configure Streamlit page
st.set_page_config(
    page_title=" Ù…Ø´Ø§ÙˆØ±Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø´ØªÙ‡",
    page_icon="ğŸ“",
    layout="wide"
)
st.markdown("""
<style>
    body, [class*="css"]  {
        direction: rtl;
        text-align: rigrtlht;
        font-family: 'B Nazanin', 'Arial', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# Cache functions to avoid reloading data and model
@st.cache_data
def load_data():
    with open("advisor_data.json", 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    questions = [item["input"] for item in data]
    answers = [item["output"] for item in data]
    
    return questions, answers, data

@st.cache_resource
def load_model():
    model_name = "HooshvareLab/bert-fa-zwnj-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    return tokenizer, model, device

@st.cache_data
def create_embeddings_and_index(_questions):
    question_embeddings = create_embeddings(_questions)
    
    dimension = question_embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    index.add(np.array(question_embeddings).astype('float32'))
    
    return question_embeddings, index

# Create embeddings for questions using a Persian language model
def create_embeddings(texts):
    tokenizer, model, device = load_model()

    if isinstance(texts, str):
        texts = [texts]

    encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}

    with torch.no_grad():
        model_output = model(**encoded_input)

    token_embeddings = model_output[0]
    attention_mask = encoded_input['attention_mask']
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sentence_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)

    return sentence_embeddings.cpu().numpy()

def retrieve_relevant_docs(query, questions, answers, index, k=5):
    """Retrieve relevant documents for RAG"""
    query_embedding = create_embeddings(query)
    scores, indices = index.search(np.array(query_embedding).astype('float32'), k)

    relevant_docs = []
    for i, idx in enumerate(indices[0]):
        if idx < len(questions) and scores[0][i] > 0.75:  # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ RAG
            relevant_docs.append({
                "question": questions[idx],
                "answer": answers[idx],
                "score": float(scores[0][i])
            })

    return relevant_docs

def call_groq_api(prompt, api_key):
    """Call Groq API for text generation"""
    url = "https://api.groq.com/openai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "llama-3.1-8b-instant",
        "messages": [
            {
                "role": "system", 
                "content": "Ø´Ù…Ø§ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± ØªØ­ØµÛŒÙ„ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ù¾Ø§Ø³Ø®ÛŒ Ù…ÙÛŒØ¯ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¨Ø§ Ù„Ù‡Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒØ¯ØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯ÙˆÛŒÛŒØ¯."
            },
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "temperature": 0.7,
        "max_tokens": 1024
    }
    
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Groq API: {str(e)}"
    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø®: {str(e)}"

def generate_rag_response(query, relevant_docs, api_key):
    """Generate response using RAG approach"""
    if not relevant_docs:
        no_context_prompt = f"""
        Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query}

        Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø´Ø§ÙˆØ±Ù‡ ØªØ­ØµÛŒÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. 
        Ø§Ú¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø´Ø§ÙˆØ±Ù‡ ØªØ­ØµÛŒÙ„ÛŒØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø´ØªÙ‡ ÛŒØ§ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ø³ØªØŒ 
        Ù„Ø·ÙØ§Ù‹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ø®ÙˆØ¯ Ù¾Ø§Ø³Ø® Ù…Ø®ØªØµØ±ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
        Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù†Ø§Ù…Ø±ØªØ¨Ø· Ø§Ø³ØªØŒ Ù…ÙˆØ¯Ø¨Ø§Ù†Ù‡ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ ÙÙ‚Ø· Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ù…Ø´Ø§ÙˆØ±Ù‡ ØªØ­ØµÛŒÙ„ÛŒ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.
        """
        return call_groq_api(no_context_prompt, api_key)
    
    # Create context from relevant documents
    context = "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡:\n\n"
    for i, doc in enumerate(relevant_docs[:3]):  
        context += f"Ø³ÙˆØ§Ù„ {i+1}: {doc['question']}\n"
        context += f"Ù¾Ø§Ø³Ø® {i+1}: {doc['answer']}\n"
        context += f"Ø§Ù…ØªÛŒØ§Ø² Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯Ù†: {doc['score']:.3f}\n\n"
    
    prompt = f"""
{context}

Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±: {query}

Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ù„Ø§ØŒ Ù„Ø·ÙØ§Ù‹ Ù¾Ø§Ø³Ø®ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
- Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø§Ø³ØªØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯
- Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ§Ø¶Ø­ , Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡  Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
- Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯ÙˆÛŒÛŒØ¯
-Ø¯Ø± Ù†Ø¸Ø± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ Ú©Ù‡ Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø§Ù†Ø´ Ø§Ù…ÙˆØ² Ù¾Ø§ÛŒÙ‡ Ù†Ù‡Ù… Ø¨Ø§Ø´Ø¯
-Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø±Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø´ØªÙ‡ Ù¾Ø§ÛŒÙ‡ Ù†Ù‡Ù… Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª 

Ù¾Ø§Ø³Ø®:
"""
    
    return call_groq_api(prompt, api_key)

# Main Streamlit App
def main():
    st.title("ğŸ“Ø³ÛŒØ³ØªÙ… Ù…Ø´Ø§ÙˆØ±Ù‡ ØªØ­ØµÛŒÙ„ÛŒ Ù¾Ø§ÛŒÙ‡ Ù†Ù‡Ù…")
    st.markdown("Ù‡Ø± Ø³ÙˆØ§Ù„ÛŒ Ø¯Ø±Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø´ØªÙ‡ Ø¯Ø§Ø±ÛŒ Ø¨Ù¾Ø±Ø³")
    st.markdown("---")
    

    api_key = GORK_API_KEY
    
    # Initialize session state
    if 'initialized' not in st.session_state:
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø¯Ù„..."):
            # Load data
            questions, answers, data = load_data()
            st.session_state.questions = questions
            st.session_state.answers = answers
            st.session_state.data = data
            
            # Load model
            load_model()
            
            # Create embeddings and index
            question_embeddings, index = create_embeddings_and_index(questions)
            st.session_state.question_embeddings = question_embeddings
            st.session_state.index = index
            st.session_state.initialized = True
            


    # Chat interface
    st.subheader("ğŸ’¬ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯")
    
    # User input
    user_query = st.text_area(
        "Ø³ÙˆØ§Ù„:",
        placeholder="Ù…Ø«Ø§Ù„: Ù…Ù† Ø¨Ù‡ Ø±ÛŒØ§Ø¶ÛŒ Ø¹Ù„Ø§Ù‚Ù‡ Ø¯Ø§Ø±Ù… Ùˆ Ø§Ø² Ø­ÙØ¸ Ú©Ø±Ø¯Ù† Ø¨Ø¯Ù… Ù…ÛŒØ§Ø¯ØŒ Ú†Ù‡ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ¯ÛŒØ¯ØŸ",
        height=100,
        key="user_input"
    )
    
    # Submit button
    if st.button("ğŸ¤– Ù¾Ø§Ø³Ø® Ù…Ø´Ø§ÙˆØ±", type="primary") or user_query:
        if user_query.strip():
            with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®..."):
                # Retrieve relevant documents
                relevant_docs = retrieve_relevant_docs(
                    user_query, 
                    st.session_state.questions, 
                    st.session_state.answers, 
                    st.session_state.index
                )
                
                # Generate RAG response
                rag_response = generate_rag_response(user_query, relevant_docs, api_key)
                
                # Display response
                st.subheader("ğŸ¤– Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ:")
                st.success(rag_response)
                
                # Display retrieved context in expander
                with st.expander("ğŸ“š Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ (Ù…Ù†Ø§Ø¨Ø¹)"):
                    if relevant_docs:
                        st.write(f"**ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§ÙØª Ø´Ø¯Ù‡:** {len(relevant_docs)}")
                        for i, doc in enumerate(relevant_docs):
                            with st.container():
                                st.markdown(f"**Ù…Ù†Ø¨Ø¹ {i+1} (Ø§Ù…ØªÛŒØ§Ø²: {doc['score']:.3f})**")
                                st.markdown(f"**Ø³ÙˆØ§Ù„:** {doc['question']}")
                                st.markdown(f"**Ù¾Ø§Ø³Ø®:** {doc['answer']}")
                                st.markdown("---")
                    else:
                        st.write("Ù‡ÛŒÚ† Ù…Ù†Ø¨Ø¹ Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
        else:
            st.warning("Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")

 
if __name__ == "__main__":
    main()